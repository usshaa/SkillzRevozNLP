{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# \ud83c\udf99\ufe0f Speech Recognition with Python\n", "This notebook demonstrates different ways of performing **speech-to-text** using:\n", "- `SpeechRecognition` (Google Web API)\n", "- Hugging Face pretrained model (**Wav2Vec2**)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Step 1: Install Dependencies"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["!pip install SpeechRecognition pydub transformers datasets torchaudio --quiet"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Step 2: Import Libraries"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import speech_recognition as sr\n", "from pydub import AudioSegment\n", "import torch\n", "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n", "import librosa\n", "import numpy as np"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Step 3: Load or Record Audio"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from google.colab import files\n", "uploaded = files.upload()  # upload your .wav or .mp3 file\n", "\n", "# Convert to wav if mp3\n", "for fn in uploaded.keys():\n", "    if fn.endswith('.mp3'):\n", "        sound = AudioSegment.from_mp3(fn)\n", "        fn_wav = fn.replace('.mp3', '.wav')\n", "        sound.export(fn_wav, format=\"wav\")\n", "        audio_path = fn_wav\n", "    else:\n", "        audio_path = fn\n", "print(\"Audio ready:\", audio_path)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Step 4: Baseline \u2013 Google SpeechRecognition API"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["recognizer = sr.Recognizer()\n", "with sr.AudioFile(audio_path) as source:\n", "    audio_data = recognizer.record(source)\n", "try:\n", "    text = recognizer.recognize_google(audio_data)\n", "    print(\"\ud83d\udd39 Recognized Text (Google API):\", text)\n", "except Exception as e:\n", "    print(\"Error:\", e)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Step 5: Deep Learning \u2013 Wav2Vec2 Pretrained Model"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n", "model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\")\n", "\n", "speech, rate = librosa.load(audio_path, sr=16000)\n", "input_values = processor(speech, return_tensors=\"pt\", sampling_rate=16000).input_values\n", "\n", "with torch.no_grad():\n", "    logits = model(input_values).logits\n", "\n", "predicted_ids = torch.argmax(logits, dim=-1)\n", "transcription = processor.batch_decode(predicted_ids)[0]\n", "print(\"\ud83d\udd39 Recognized Text (Wav2Vec2):\", transcription)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Step 6: Evaluation (Word Error Rate)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from datasets import load_metric\n", "wer_metric = load_metric(\"wer\")\n", "\n", "# if ground truth available\n", "ground_truth = \"this is a sample sentence\"\n", "wer = wer_metric.compute(predictions=[transcription.lower()], references=[ground_truth.lower()])\n", "print(f\"WER: {wer:.2f}\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## \u2705 Future Enhancements\n", "- Train on custom dataset\n", "- Add real-time microphone recording\n", "- Deploy as a Flask or Streamlit app"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "version": "3.10"}}, "nbformat": 4, "nbformat_minor": 2}