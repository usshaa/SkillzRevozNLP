{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "90547aeb",
      "metadata": {
        "id": "90547aeb"
      },
      "outputs": [],
      "source": [
        "# Install required libraries (if not already installed)\n",
        "!pip install --quiet nltk wordcloud requests beautifulsoup4 scikit-learn\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "6a1f99f6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6a1f99f6",
        "outputId": "9572d0d1-cc2a-4220-9ff8-46ddb959d9e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n",
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker_tab to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "nltk.download('vader_lexicon')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "nltk.download('maxent_ne_chunker_tab')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "46229d95",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "46229d95",
        "outputId": "435f4330-b76e-4e13-ab40-d218949c6937"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Newsgroups sample: I was wondering if anyone out there could enlighten me on this car I saw\n",
            "the other day. It was a 2-door sports car, looked to be from the late 60s/\n",
            "early 70s. It was called a Bricklin. The doors were really small. In addition,\n",
            "the front bumper was separate from the rest of the body. This is \n",
            "all I k \n",
            "\n",
            "SMS sample:\n",
            "   label                                            message\n",
            "0   ham  Go until jurong point, crazy.. Available only ...\n",
            "1   ham                      Ok lar... Joking wif u oni...\n",
            "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
            "3   ham  U dun say so early hor... U c already then say...\n",
            "4   ham  Nah I don't think he goes to usf, he lives aro...\n"
          ]
        }
      ],
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "import pandas as pd\n",
        "import requests\n",
        "from io import StringIO\n",
        "\n",
        "# 20 Newsgroups dataset (subset)\n",
        "newsgroups = fetch_20newsgroups(subset='train', remove=('headers','footers','quotes'))\n",
        "texts = newsgroups.data[:3]  # small sample\n",
        "\n",
        "# SMS Spam dataset\n",
        "url = 'https://raw.githubusercontent.com/justmarkham/pycon-2016-tutorial/master/data/sms.tsv'\n",
        "resp = requests.get(url)\n",
        "sms_df = pd.read_csv(StringIO(resp.text), sep='\\t', names=['label', 'message'])\n",
        "\n",
        "print(\"Newsgroups sample:\", texts[0][:300], \"\\n\")\n",
        "print(\"SMS sample:\\n\", sms_df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import os\n",
        "\n",
        "try:\n",
        "    punkt_tab_path = nltk.data.find('tokenizers/punkt_tab/english/')\n",
        "    print(f\"punkt_tab resource found at: {punkt_tab_path}\")\n",
        "except LookupError:\n",
        "    print(\"punkt_tab resource not found in NLTK data paths.\")\n",
        "\n",
        "print(\"\\nNLTK data paths:\")\n",
        "for path in nltk.data.path:\n",
        "    print(path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ZsIooDMEL0i",
        "outputId": "0bcb3fd0-b0bb-4f08-a689-ad4cd6ddd9bb"
      },
      "id": "7ZsIooDMEL0i",
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "punkt_tab resource found at: /root/nltk_data/tokenizers/punkt_tab/english\n",
            "\n",
            "NLTK data paths:\n",
            "/root/nltk_data\n",
            "/usr/nltk_data\n",
            "/usr/share/nltk_data\n",
            "/usr/lib/nltk_data\n",
            "/usr/share/nltk_data\n",
            "/usr/local/share/nltk_data\n",
            "/usr/lib/nltk_data\n",
            "/usr/local/lib/nltk_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "95a6a792",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "95a6a792",
        "outputId": "73a61f7f-addc-4417-8d23-0bafc66b4fb9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original: Hello, Usha! NLP is FUN, isn't it?\n",
            "Lowercased: hello, usha! nlp is fun, isn't it?\n",
            "Without punctuation: hello usha nlp is fun isnt it\n"
          ]
        }
      ],
      "source": [
        "import re, string\n",
        "\n",
        "text = \"Hello, Usha! NLP is FUN, isn't it?\"\n",
        "\n",
        "# Lowercasing\n",
        "lower = text.lower()\n",
        "\n",
        "# Remove punctuation\n",
        "clean = lower.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "print(\"Original:\", text)\n",
        "print(\"Lowercased:\", lower)\n",
        "print(\"Without punctuation:\", clean)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "edb5e0a3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "edb5e0a3",
        "outputId": "07a85c26-fbb4-45fe-b9a8-e03b8a0cde8e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence tokens: ['Usha is learning NLP.', 'She loves tokenization!']\n",
            "Word tokens: ['Usha', 'is', 'learning', 'NLP', '.', 'She', 'loves', 'tokenization', '!']\n"
          ]
        }
      ],
      "source": [
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "import nltk\n",
        "\n",
        "# Ensure punkt_tab is downloaded before use\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt_tab/english/')\n",
        "except LookupError:\n",
        "    nltk.download('punkt_tab')\n",
        "\n",
        "\n",
        "text = \"Usha is learning NLP. She loves tokenization!\"\n",
        "\n",
        "print(\"Sentence tokens:\", sent_tokenize(text))\n",
        "print(\"Word tokens:\", word_tokenize(text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "3c278b05",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3c278b05",
        "outputId": "35d749b2-2b6a-4f9a-ba5a-2c7e7518460b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original tokens: ['Usha', 'is', 'learning', 'Natural', 'Language', 'Processing', 'in', 'Python', '.']\n",
            "After stopword removal: ['Usha', 'learning', 'Natural', 'Language', 'Processing', 'Python']\n"
          ]
        }
      ],
      "source": [
        "from nltk.corpus import stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "text = \"Usha is learning Natural Language Processing in Python.\"\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "filtered = [w for w in tokens if w.lower() not in stop_words and w.isalpha()]\n",
        "\n",
        "print(\"Original tokens:\", tokens)\n",
        "print(\"After stopword removal:\", filtered)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "afbf344b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "afbf344b",
        "outputId": "70dbc19c-4286-4621-ed1b-d757fae89c70"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stemming:\n",
            "running -> run\n",
            "runs -> run\n",
            "runner -> runner\n",
            "better -> better\n",
            "easily -> easili\n",
            "\n",
            "Lemmatization (noun default):\n",
            "running -> running\n",
            "runs -> run\n",
            "runner -> runner\n",
            "better -> better\n",
            "easily -> easily\n"
          ]
        }
      ],
      "source": [
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "ps = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "words = ['running', 'runs', 'runner', 'better', 'easily']\n",
        "\n",
        "print(\"Stemming:\")\n",
        "for w in words:\n",
        "    print(w, \"->\", ps.stem(w))\n",
        "\n",
        "print(\"\\nLemmatization (noun default):\")\n",
        "for w in words:\n",
        "    print(w, \"->\", lemmatizer.lemmatize(w))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "36387f21",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "36387f21",
        "outputId": "dee91246-eebb-41eb-a4b9-bd3f3698b3a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Usha', 'NNP'), ('is', 'VBZ'), ('learning', 'VBG'), ('Natural', 'NNP'), ('Language', 'NNP'), ('Processing', 'NNP'), ('.', '.')]\n"
          ]
        }
      ],
      "source": [
        "text = \"Usha is learning Natural Language Processing.\"\n",
        "tokens = word_tokenize(text)\n",
        "pos_tags = nltk.pos_tag(tokens)\n",
        "print(pos_tags)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "178ef599",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "178ef599",
        "outputId": "6bb231de-9510-4ca0-889e-c1a19da12192"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entities: [('Apple', 'GPE'), ('London', 'GPE')]\n"
          ]
        }
      ],
      "source": [
        "from nltk import ne_chunk\n",
        "\n",
        "text = \"Apple is buying a startup in London for $1 billion.\"\n",
        "tokens = word_tokenize(text)\n",
        "pos_tags = nltk.pos_tag(tokens)\n",
        "tree = ne_chunk(pos_tags)\n",
        "\n",
        "entities = []\n",
        "for subtree in tree:\n",
        "    if hasattr(subtree, 'label'):\n",
        "        entity_name = \" \".join([token for token, pos in subtree.leaves()])\n",
        "        entities.append((entity_name, subtree.label()))\n",
        "\n",
        "print(\"Entities:\", entities)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "5f2abcd7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5f2abcd7",
        "outputId": "9211a1c9-d40b-4cc6-bff7-1e32177649a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence1: Synset('savings_bank.n.02')\n",
            "Sentence2: Synset('deposit.v.02')\n"
          ]
        }
      ],
      "source": [
        "from nltk.wsd import lesk\n",
        "\n",
        "sent1 = \"I went to the bank to deposit money\"\n",
        "sent2 = \"The river bank was full of fishers\"\n",
        "\n",
        "print(\"Sentence1:\", lesk(sent1.split(), 'bank'))\n",
        "print(\"Sentence2:\", lesk(sent2.split(), 'bank'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "b2f05f50",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b2f05f50",
        "outputId": "d7515968-e78b-42fd-ca12-8ba49ed426fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Message: Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...\n",
            "Tokens, Stems, Lemmas: (['go', 'jurong', 'point', 'crazy', 'available', 'bugis', 'n', 'great', 'world', 'la', 'e', 'buffet', 'cine', 'got', 'amore', 'wat'], ['go', 'jurong', 'point', 'crazi', 'avail', 'bugi', 'n', 'great', 'world', 'la', 'e', 'buffet', 'cine', 'got', 'amor', 'wat'], ['go', 'jurong', 'point', 'crazy', 'available', 'bugis', 'n', 'great', 'world', 'la', 'e', 'buffet', 'cine', 'got', 'amore', 'wat'])\n",
            "\n",
            "Message: Ok lar... Joking wif u oni...\n",
            "Tokens, Stems, Lemmas: (['ok', 'lar', 'joking', 'wif', 'u', 'oni'], ['ok', 'lar', 'joke', 'wif', 'u', 'oni'], ['ok', 'lar', 'joking', 'wif', 'u', 'oni'])\n",
            "\n",
            "Message: Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\n",
            "Tokens, Stems, Lemmas: (['free', 'entry', 'wkly', 'comp', 'win', 'fa', 'cup', 'final', 'tkts', 'may', 'text', 'fa', 'receive', 'entry', 'questionstd', 'txt', 'ratetcs', 'apply'], ['free', 'entri', 'wkli', 'comp', 'win', 'fa', 'cup', 'final', 'tkt', 'may', 'text', 'fa', 'receiv', 'entri', 'questionstd', 'txt', 'ratetc', 'appli'], ['free', 'entry', 'wkly', 'comp', 'win', 'fa', 'cup', 'final', 'tkts', 'may', 'text', 'fa', 'receive', 'entry', 'questionstd', 'txt', 'ratetcs', 'apply'])\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "ps = PorterStemmer()\n",
        "\n",
        "def preprocess(text):\n",
        "    t = text.lower()\n",
        "    t = t.translate(str.maketrans('', '', string.punctuation))\n",
        "    tokens = word_tokenize(t)\n",
        "    tokens = [w for w in tokens if w.isalpha() and w not in stop_words]\n",
        "    stems = [ps.stem(w) for w in tokens]\n",
        "    lemmas = [lemmatizer.lemmatize(w) for w in tokens]\n",
        "    return tokens, stems, lemmas\n",
        "\n",
        "for i, row in sms_df.head(3).iterrows():\n",
        "    print(\"Message:\", row['message'])\n",
        "    print(\"Tokens, Stems, Lemmas:\", preprocess(row['message']))\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8ar907pTEEdy"
      },
      "id": "8ar907pTEEdy",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}