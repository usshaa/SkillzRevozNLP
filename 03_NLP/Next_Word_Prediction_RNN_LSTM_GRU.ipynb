{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "0ceb7a84",
      "metadata": {
        "id": "0ceb7a84"
      },
      "source": [
        "# Next Word Prediction with RNN, LSTM, and GRU\n",
        "\n",
        "**how sequence models predict the next word in a sentence**.\n",
        "\n",
        "We will:\n",
        "- Load a small text corpus (Shakespeare sample).\n",
        "- Preprocess text into input-output word sequences.\n",
        "- Train and compare SimpleRNN, LSTM, and GRU models.\n",
        "- Test models by typing partial sentences to see predictions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3af85e4d",
      "metadata": {
        "id": "3af85e4d"
      },
      "source": [
        "## 1) Setup and Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "bffc611f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bffc611f",
        "outputId": "d1b2a1b5-a687-43dd-f2f0-c414634c752a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow version: 2.19.0\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, SimpleRNN, LSTM, GRU, Dense\n",
        "\n",
        "print('TensorFlow version:', tf.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "629d63b9",
      "metadata": {
        "id": "629d63b9"
      },
      "source": [
        "## 2) Load Sample Text Corpus\n",
        "We’ll use a few lines from Shakespeare for demonstration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "49b615e9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "49b615e9",
        "outputId": "75b903bf-f4c3-41b8-d0f6-59743802babb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample lines:\n",
            "- To be or not to be that is the question\n",
            "- All the world is a stage and all the men and women merely players\n",
            "- Some are born great some achieve greatness and some have greatness thrust upon them\n",
            "- The course of true love never did run smooth\n",
            "- If music be the food of love play on\n"
          ]
        }
      ],
      "source": [
        "data = [\n",
        "    \"To be or not to be that is the question\",\n",
        "    \"All the world is a stage and all the men and women merely players\",\n",
        "    \"Some are born great some achieve greatness and some have greatness thrust upon them\",\n",
        "    \"The course of true love never did run smooth\",\n",
        "    \"If music be the food of love play on\"\n",
        "]\n",
        "\n",
        "print(\"Sample lines:\")\n",
        "for line in data:\n",
        "    print('-', line)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ac40a6f9",
      "metadata": {
        "id": "ac40a6f9"
      },
      "source": [
        "## 3) Tokenize Text and Create Sequences\n",
        "We create input-output pairs where X = sequence of words, y = next word."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "45934213",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "45934213",
        "outputId": "158c3ae6-f7c8-494a-b386-f1ca976b652d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 41\n",
            "Total sequences: 51\n",
            "X shape: (51, 13) y shape: (51, 41)\n"
          ]
        }
      ],
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(data)\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print('Vocabulary size:', vocab_size)\n",
        "\n",
        "sequences = []\n",
        "for line in data:\n",
        "    tokens = tokenizer.texts_to_sequences([line])[0]\n",
        "    for i in range(1, len(tokens)):\n",
        "        seq = tokens[:i+1]\n",
        "        sequences.append(seq)\n",
        "\n",
        "print('Total sequences:', len(sequences))\n",
        "\n",
        "# Pad sequences\n",
        "maxlen = max(len(seq) for seq in sequences)\n",
        "sequences = pad_sequences(sequences, maxlen=maxlen, padding='pre')\n",
        "\n",
        "X, y = sequences[:,:-1], sequences[:,-1]\n",
        "y = tf.keras.utils.to_categorical(y, num_classes=vocab_size)\n",
        "\n",
        "print('X shape:', X.shape, 'y shape:', y.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f8b10150",
      "metadata": {
        "id": "f8b10150"
      },
      "source": [
        "## 4) Build Model Function\n",
        "We can create SimpleRNN, LSTM, or GRU models with the same architecture."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "bf382ef9",
      "metadata": {
        "id": "bf382ef9"
      },
      "outputs": [],
      "source": [
        "def build_model(model_type='RNN', vocab_size=100, embed_dim=50, maxlen=10, rnn_units=64):\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(vocab_size, embed_dim, input_length=maxlen))\n",
        "    if model_type == 'RNN':\n",
        "        model.add(SimpleRNN(rnn_units))\n",
        "    elif model_type == 'LSTM':\n",
        "        model.add(LSTM(rnn_units))\n",
        "    elif model_type == 'GRU':\n",
        "        model.add(GRU(rnn_units))\n",
        "    model.add(Dense(vocab_size, activation='softmax'))\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa7fef1f",
      "metadata": {
        "id": "fa7fef1f"
      },
      "source": [
        "## 5) Train Models\n",
        "We will train each model briefly (small dataset, few epochs for demo)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "b263f952",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b263f952",
        "outputId": "6f54846c-a9c2-43c1-97ec-0baa8292a61d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training SimpleRNN...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training LSTM...\n",
            "Training GRU...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7a9838158f20>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "EPOCHS = 200  # longer since dataset is small\n",
        "\n",
        "model_rnn = build_model('RNN', vocab_size, embed_dim=50, maxlen=maxlen, rnn_units=64)\n",
        "model_lstm = build_model('LSTM', vocab_size, embed_dim=50, maxlen=maxlen, rnn_units=64)\n",
        "model_gru = build_model('GRU', vocab_size, embed_dim=50, maxlen=maxlen, rnn_units=64)\n",
        "\n",
        "print('Training SimpleRNN...')\n",
        "model_rnn.fit(X, y, epochs=EPOCHS, verbose=0)\n",
        "print('Training LSTM...')\n",
        "model_lstm.fit(X, y, epochs=EPOCHS, verbose=0)\n",
        "print('Training GRU...')\n",
        "model_gru.fit(X, y, epochs=EPOCHS, verbose=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ef4874a",
      "metadata": {
        "id": "9ef4874a"
      },
      "source": [
        "## 6) Generate Text Function\n",
        "We’ll write a function to input a seed text and predict the next words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "9ce5bd5d",
      "metadata": {
        "id": "9ce5bd5d"
      },
      "outputs": [],
      "source": [
        "def generate_text(model, tokenizer, seed_text, num_words=5, maxlen=10):\n",
        "    result = seed_text\n",
        "    for _ in range(num_words):\n",
        "        token_list = tokenizer.texts_to_sequences([result])[0]\n",
        "        token_list = pad_sequences([token_list], maxlen=maxlen, padding='pre')\n",
        "        predicted = np.argmax(model.predict(token_list, verbose=0), axis=-1)[0]\n",
        "        for word, index in tokenizer.word_index.items():\n",
        "            if index == predicted:\n",
        "                result += ' ' + word\n",
        "                break\n",
        "    return result"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0f6287cb",
      "metadata": {
        "id": "0f6287cb"
      },
      "source": [
        "## 7) Compare Outputs\n",
        "Now try generating text with each model and compare results."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"RNN:\", generate_text(model_rnn, tokenizer, \"to be\", 5))\n",
        "print(\"LSTM:\", generate_text(model_lstm, tokenizer, \"to be\", 5))\n",
        "print(\"GRU:\", generate_text(model_gru, tokenizer, \"to be\", 5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FBfa6VKkpMja",
        "outputId": "87b1adb7-1737-4d0f-d735-808e544fc9fd"
      },
      "id": "FBfa6VKkpMja",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RNN: to be or not to be that\n",
            "LSTM: to be or not is is the\n",
            "GRU: to be or not to be that\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "223030e6",
      "metadata": {
        "id": "223030e6"
      },
      "source": [
        "## Teaching Notes\n",
        "- RNN struggles with long-term dependencies.\n",
        "- LSTM and GRU remember longer contexts better.\n",
        "- This small demo corpus exaggerates differences but illustrates concepts.\n",
        "\n",
        "## Conclusion\n",
        "This example shows how RNN-family models can **generate sequences**. This intuition sets the stage for **Transformers (BERT, GPT)** which model sequences with attention instead of recurrence."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}