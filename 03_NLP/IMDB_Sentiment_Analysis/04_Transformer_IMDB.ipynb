{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f17f99a",
   "metadata": {},
   "source": [
    "# Transformer Encoder for IMDB Sentiment Classification\n",
    "\n",
    "A small Transformer encoder built with Keras (MultiHeadAttention) used as a text classifier. This demonstrates attention and parallel sequence processing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "789e3add",
   "metadata": {},
   "source": [
    "## 0. Environment / Install (run if needed)\n",
    "Run this cell to install packages if they are missing. On Colab you can skip already installed ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1c5402",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print('Python', sys.version)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28578588",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b47ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "import numpy as np\n",
    "\n",
    "# Load raw IMDB (using tensorflow datasets would require extra install; we'll use keras's imdb indices)\n",
    "from tensorflow.keras.datasets import imdb\n",
    "vocab_size = 10000\n",
    "maxlen = 200\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=vocab_size)\n",
    "\n",
    "# Convert integer sequences back to text using imdb.get_word_index (for vectorization demo)\n",
    "word_index = imdb.get_word_index()\n",
    "index_word = {i+3: w for w, i in word_index.items()}\n",
    "index_word[0] = '<pad>'; index_word[1] = '<start>'; index_word[2] = '<unk>'\n",
    "\n",
    "def decode_review(seq):\n",
    "    return ' '.join(index_word.get(i, '?') for i in seq)\n",
    "\n",
    "x_train_text = [decode_review(s) for s in x_train]\n",
    "x_test_text = [decode_review(s) for s in x_test]\n",
    "\n",
    "print('Example:', x_train_text[0][:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11897ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text vectorization\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "vectorizer = TextVectorization(max_tokens=vocab_size, output_sequence_length=maxlen)\n",
    "vectorizer.adapt(x_train_text)\n",
    "\n",
    "# Build simple transformer encoder block\n",
    "from tensorflow.keras.layers import Embedding, LayerNormalization, Dense, Dropout, MultiHeadAttention, GlobalAveragePooling1D, Input\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "embedding_dim = 64\n",
    "num_heads = 4\n",
    "ff_dim = 128\n",
    "\n",
    "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0.1):\n",
    "    # Attention + skip\n",
    "    x = MultiHeadAttention(key_dim=head_size, num_heads=num_heads)(inputs, inputs)\n",
    "    x = Dropout(dropout)(x)\n",
    "    x = LayerNormalization(epsilon=1e-6)(x)\n",
    "    res = x + inputs\n",
    "\n",
    "    # Feed-forward\n",
    "    x = Dense(ff_dim, activation='relu')(res)\n",
    "    x = Dropout(dropout)(x)\n",
    "    x = Dense(inputs.shape[-1])(x)\n",
    "    x = LayerNormalization(epsilon=1e-6)(x)\n",
    "    return x + res\n",
    "\n",
    "inputs = Input(shape=(None,), dtype='int64')\n",
    "x = Embedding(vocab_size, embedding_dim)(inputs)\n",
    "x = transformer_encoder(x, head_size=embedding_dim//num_heads, num_heads=num_heads, ff_dim=ff_dim)\n",
    "x = GlobalAveragePooling1D()(x)\n",
    "x = Dropout(0.2)(x)\n",
    "outputs = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = Model(inputs, outputs)\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0683920e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare vectorized integer sequences\n",
    "x_train_vect = vectorizer(np.array(x_train_text))\n",
    "x_test_vect = vectorizer(np.array(x_test_text))\n",
    "\n",
    "# Train (short for demo)\n",
    "history = model.fit(x_train_vect, y_train, epochs=3, batch_size=128, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d275e57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, acc = model.evaluate(x_test_vect, y_test)\n",
    "print(f'Test accuracy: {acc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eabf51c",
   "metadata": {},
   "source": [
    "## Notes\n",
    "- Transformer uses attention to capture relationships across entire sequence. This demo uses a single encoder block."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
